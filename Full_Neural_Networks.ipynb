{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full Neural Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFY3QR7iEry1e7X62w4fye",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/123-code/Neural-Networks-from-scratch/blob/main/Full_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBeWQv78cvxr",
        "outputId": "e4737d05-7a09-40d9-8ab1-c83cab6de41e"
      },
      "source": [
        "import numpy as np\n",
        "!pip install nnfs\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdhWHvN6c90G",
        "outputId": "fb680f03-94ff-4325-c0f2-cd11e2e056f1"
      },
      "source": [
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self,inputs,nneurons):\n",
        "    self.weights = 0.10 * np.random.randn(inputs,nneurons)\n",
        "    self.bias = np.zeros((1,nneurons))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "      self.output = np.dot(inputs,self.weights) + self.bias\n",
        "     \n",
        "\n",
        "\n",
        "class RELU_A:\n",
        "  def forward(self,inputs):\n",
        "\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "  def forward(self,inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
        "    probabilities = exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
        "    self.output = probabilities\n",
        "\n",
        "class Loss:\n",
        "  # output from the model and target values.\n",
        "  def calculate(self,output,y):\n",
        "    sample_losses = self.forward(output,y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class categoricalcrossentropy(Loss):\n",
        "  def forward(self,y_pred,y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
        "\n",
        "    if len(y_true.shape)==1:\n",
        "      correct_conf = y_pred_clipped[range(samples),y_true]\n",
        "    elif len(y_true.shape)==2:\n",
        "      correct_conf = np.sum(y_pred_clipped*y_true,axis=1)\n",
        "    \n",
        "    negative_log_likelihoods = np.log(correct_conf)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "\n",
        "\n",
        "X,y = spiral_data(samples=100,classes=3)\n",
        "dense1 = Layer(2,3)\n",
        "activation1 = RELU_A()\n",
        "\n",
        "dense2 = Layer(3,3)\n",
        "activation2 = Softmax()\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "print(activation2.output[:5])\n",
        "Loss = categoricalcrossentropy()\n",
        "loss = Loss.calculate(activation2.output,y)\n",
        "print(\"Loss:\",loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33331734 0.3333183  0.33336434]\n",
            " [0.3332888  0.33329153 0.33341965]\n",
            " [0.33325943 0.33326396 0.33347666]\n",
            " [0.33323312 0.33323926 0.33352762]]\n",
            "Loss: -1.098445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFMRKLLzaIVj"
      },
      "source": [
        "Neural Network 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCLrh13-aKhp",
        "outputId": "35cbc5cd-5815-463c-94db-c4876ca76bca"
      },
      "source": [
        "data = [[2,5,8,9],[6,8,3,1],[7,9,4,5],[6,4,2,1]]\n",
        "#np.random.seed(0)\n",
        "# setting initial weights and biases. \n",
        "class Layer1:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = 0.10 * np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases = np.zeros((1,n_neurons))\n",
        "\n",
        "  def Forward_pass(self,inputs):\n",
        "    self.output = np.dot(inputs,self.weights) + self.biases\n",
        "# Values printed in output are a matrix of each neurons dot product result. \n",
        "\n",
        "class RELU:\n",
        "  def forward(self,inputs):\n",
        "\n",
        "    self.output = np.maximum(0,inputs)\n",
        "  \n",
        "\n",
        "L1 = Layer1(4,3)\n",
        "L1.Forward_pass(data)\n",
        "print(L1.output)\n",
        "activation_1 = RELU()\n",
        "activation_1.forward(L1.output)\n",
        "print(activation_1.output[:5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.48369895 1.06012667 2.91633185]\n",
            " [1.78269892 0.50147318 3.0249836 ]\n",
            " [1.97679322 0.88927948 3.86635385]\n",
            " [1.4470814  0.76571782 2.16623242]]\n",
            "[[0.48369895 1.06012667 2.91633185]\n",
            " [1.78269892 0.50147318 3.0249836 ]\n",
            " [1.97679322 0.88927948 3.86635385]\n",
            " [1.4470814  0.76571782 2.16623242]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uitg8vBbAm_v"
      },
      "source": [
        "softmax_outputs = np.array([0.7,0.1,0.2],\n",
        "                           [0.1,0.5,0.4],\n",
        "                           [0.02,0.9,0.08])\n",
        "\n",
        "class_targets = [0,1,1]\n",
        "print(softmax_outputs[0,1,2],class_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwV-QyNKE90J"
      },
      "source": [
        "Full NN 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-_jGd7RFAeN"
      },
      "source": [
        "def sigmoid(x):\n",
        "  sigmoid = (1/(1+np.exp(-x)))\n",
        "  return sigmoid\n",
        "\n",
        "# randomly initializing weights.\n",
        "def init_W(inp_size,hidd_size):\n",
        "  weights = {}\n",
        "  weights['W1'] = np.random.randn(inp_size,hidd_size)\n",
        "  weights['B1'] = np.random.randn(1,hidd_size)\n",
        "  weights['W2'] = np.random.randn(inp_size,hidd_size)\n",
        "  weights['B2'] = np.random.randn(1,hidd_size)\n",
        "  return weights\n",
        "\n",
        "def Forward_Loss(X,y,weights):\n",
        "  W_sum = np.dot(X,weights['W1'])\n",
        "  output = W_sum + weights['B1']\n",
        "  Activated1 = np.dot(W_sum,output)\n",
        "\n",
        "  W_sum2 = np.dot(Activated1,weights['W2'])\n",
        "  output2 = W_sum + weights['B1']\n",
        "\n",
        "  loss = np.mean((y-output2)**2)\n",
        "\n",
        "# saving forward pass data.\n",
        "  forward_info = {}\n",
        "  forward_info['W_sum1'] = W_sum\n",
        "  forward_info['output1'] = output\n",
        "  forward_info['Sigm1'] = Activated1\n",
        "  forward_info['Wsum2'] = W_sum2\n",
        "  forward_info['output2'] = output2\n",
        "  forward_info['X'] = X\n",
        "  forward_info['y'] = y\n",
        "\n",
        "  return forward_info,loss\n",
        "\n",
        "def Calc_gradients(forward_info,weights):\n",
        "  '''\n",
        "  #Compute the partial derivatives of the loss with respect to each of the parameters in the neural network.\n",
        "  this is done to see how the error changes with respect to changes in any part of the network.\n",
        "  '''\n",
        "  dldp = -(forward_info['y'] - forward_info['output2'])\n",
        "  dpdw2 = np.ones_like(forward_info['Wsum2'])\n",
        "  dlw2 = dldp * dpdw2\n",
        "  dpdb2 = np.ones_like(weights['B2'])\n",
        "  dldb2 = (dpdb2 * dldp).sum(axis=0)\n",
        "  dp2dW2 = np.transpose(forward_info['Sigm1'], (1, 0))\n",
        "  dLdW2 = np.dot(dM2dW2, dldp)\n",
        "  dpdsigm = np.transpose(weights['W2'], (1, 0)) \n",
        "  dldsigm = np.dot(dpdsigm,dpdsigm)\n",
        "  dSigm1dO1 = sigmoid(forward_info['output1']) * (1- sigmoid(forward_info['output1']))\n",
        "  dldO1 = dldsigm * dlSigm\n",
        "  dO1dB1 = np.ones_like(weights['B1'])\n",
        "  dN1dM1 = np.ones_like(forward_info['W_sum1'])  \n",
        "  dLdB1 = (dLdO1 * dO1b1).sum(axis=0)\n",
        "  dWs1dW1 = np.transpose(forward_info['X'], (1, 0)) \n",
        "  dLdW = np.dot(dws1dw1, dLdB1)\n",
        "\n",
        "\n",
        "  Loss_Gradients = {}\n",
        "  Loss_Gradients['W2'] = dLdW2\n",
        "  Loss_Gradients['B1'] = dLdB1.sum(axis=0)\n",
        "  Loss_Gradients['W1'] = dLdW\n",
        "  Loss_Gradients['B2'] = dldb2\n",
        "\n",
        "def predict(X,weights):\n",
        "  L1 = np.dot(X,weights['W1'])\n",
        "  output1 = L1 + weights['B1']\n",
        "\n",
        "  LR1 = sigmoid(output1)\n",
        "\n",
        "  L2 = np.dot(LR1,weights['W2'])\n",
        "\n",
        "# Final prediction.\n",
        "  LR2 = L2 + weights['B2']\n",
        "'''\n",
        "  loss = (np.mean(forward_info['y'] - LR2))**2\n",
        "'''\n",
        "  return LR2\n",
        "\n",
        "predictions = predict(X,weights)\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTH2xqeZUHMD"
      },
      "source": [
        "from numpy import ndarray\n",
        "\n",
        "def assert_same_shape(arr1,arr2):\n",
        "  assert arr1.shape() == arr2.shape()\n",
        "  return None\n",
        "\n",
        "class Operations(object):\n",
        "  def __init__ (self):\n",
        "    pass\n",
        "\n",
        "  def forward(self,input:ndarray):\n",
        "    self.input = input\n",
        "    self.output = self.output()\n",
        "\n",
        "    return self.output\n",
        "\n",
        "  def backward(self,output_grad:ndarray):\n",
        "    assert_same_shape(self.output,output_grad)\n",
        "\n",
        "    self.input_grad = self.input_grad(output_grad)\n",
        "\n",
        "    assert_same_shape(self.input_grad,self.output_grad)\n",
        "\n",
        "    return self.input_grad\n",
        "\n",
        "  def output(self):\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def input_grad(self,output_grad):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "class ParameterOperation(Operations):\n",
        "  def __init__ (self,param):\n",
        "    super().__init__()\n",
        "    self.param=param\n",
        "\n",
        "  def backward(self,output_grad):\n",
        "    assert_same_shape(self.output,output_grad)\n",
        "\n",
        "    self.input_grad = self.input_grad(output_grad)\n",
        "    self.param_grad = self.param_grad(output_grad)\n",
        "\n",
        "    assert_same_shape(self.input_grad,self.input)\n",
        "    assert_same_shape(self.param,self.param_grad)\n",
        "\n",
        "  def param_grad(self,output_grad): \n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvEHGNuzWi4a"
      },
      "source": [
        "class WeightMultiply(ParameterOperation):\n",
        "  def __init__(self,W):\n",
        "    super().__init__(W)\n",
        "\n",
        "\n",
        "  def _output(self):\n",
        "    return(np.dot(self.input_,self.param))\n",
        "\n",
        "\n",
        "  def _input_grad(self,output_grad):\n",
        "    return(np.dot(output_grad,np.transpose(self.param,(1,0))))\n",
        "\n",
        "  def _param_grad(self,output_grad):\n",
        "    return(np.dot(np.transpose(self.input_,(1,0))),output_grad)\n",
        "     \n",
        "\n",
        "class BiasAddition(ParameterOperation):\n",
        "  def __init__ (self,B):\n",
        "    assert B.shspe[0] == 1\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "  def _output(self):\n",
        "    return self.input_ + self.param\n",
        "\n",
        "  def _input_grad(self,output_grad):\n",
        "    return np.ones_like(self.input_) * output_grad\n",
        "\n",
        "  def param_grad(self,output_grad):\n",
        "    param_grad = np.ones_like(self.input_)*output_grad\n",
        "    return(np.sum(param_grad,axis=0).reshape(1,param_grad.shape[1]))\n",
        "\n",
        "  \n",
        "\n",
        "class Sigmoid(Operations):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def _output(self):\n",
        "    return(1.0/1.0+np.exp(-1.0*self.input_))\n",
        "\n",
        "  def _input_grad(self,output_grad):\n",
        "    sigmoid_backward = self.output*(1.0-self.output)\n",
        "    input_grad = sigmoid_backward*output_grad\n",
        "    return input_grad\n",
        "\n",
        "\n",
        "class Layer(object):\n",
        "  def __init__ (self,neurons):\n",
        "\n",
        "    self.neurons = neurons\n",
        "    self.first = True\n",
        "    self.params = []\n",
        "    self.param_grads = []\n",
        "    self.operations = []\n",
        "\n",
        "  def setup_layer(self,num):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "  def forward(self,input_):\n",
        "    if self.first:\n",
        "      self.setup_layer(input_)\n",
        "      self.first = False\n",
        "\n",
        "    self.input_ = input_\n",
        "\n",
        "    for operation in self.operations:\n",
        "      input_ = operation.forward(input_)\n",
        "\n",
        "    self.output = input_\n",
        "\n",
        "    return self.output\n",
        "\n",
        "\n",
        "  def backward(self,output_grad):\n",
        "    assert_same_shape(self.output,output_grad)\n",
        "\n",
        "    for operation in reversed(self.operations):\n",
        "      output_grad = operation.backward(output_grad)\n",
        "\n",
        "    input_grad = output_grad\n",
        "\n",
        "    self.param_grads()\n",
        "\n",
        "    return input_grad\n",
        "\n",
        "def param_grads(self):\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V9VeEhyhIHG"
      },
      "source": [
        "import numpy as np\n",
        "inputs = np.array([2,8,7,5,4,3])\n",
        "weights = np.random.randn(6,1)*0.10\n",
        "biases = np.ones_like(inputs)\n",
        "expected = np.array([3,5,7,8,2,1])\n",
        "lr = 0.1\n",
        "\n",
        "\n",
        "class NN:\n",
        "  def __init__ (self,inputs,weights,biases,expected,lr):\n",
        "    self.inputs = inputs\n",
        "    self.weights = weights\n",
        "    self.biases = biases\n",
        "    self.expected = expected\n",
        "    self.lr = lr\n",
        "\n",
        "  def forward(self,inputs,weights):\n",
        "    for i in range(10):\n",
        "\n",
        "      output = np.dot(self.inputs,self.weights)\n",
        "      loss = ((expected - output)**2).mean()\n",
        "      n = self.inputs.shape[0]\n",
        "      gradient = 2*(expected - output)/n\n",
        "      self.weights = self.weights - (self.lr * gradient)\n",
        "\n",
        "      print(f'Error:{loss} --- Prediction:{output}')\n",
        "    \n",
        "\n",
        "      \n",
        "\n",
        "\n",
        " \n",
        "    \n",
        "\n",
        "n1 = NN(inputs,weights,biases,expected,lr) \n",
        "pred = n1.forward(inputs,weights)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}